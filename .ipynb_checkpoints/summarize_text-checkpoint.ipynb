{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f8d361-1615-4da0-a6d6-59848e338ba9",
   "metadata": {},
   "source": [
    "# Абстрактивная суммаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd3608-5e6c-42f7-bada-463d68d7b1de",
   "metadata": {},
   "source": [
    "### Контекст\n",
    "\n",
    "Необходимо провести суммаризацию полученной информации методами абстрактивной сумммаризации.\n",
    "\n",
    "Абстрактивная суммаризация – это метод автоматического реферирования текста, при котором создается новое краткое изложение с использованием перефразирования и генерации новых предложений, а не просто извлечения фрагментов из исходного текста.\n",
    "\n",
    "### Что нужно сделать?\n",
    "\n",
    "Попробовать разыне методы абстрактивной суммаризации:\n",
    "\n",
    "- T5 (Text-to-Text Transfer Transformer) - https://github.com/google-research/text-to-text-transfer-transformer\n",
    "- BART https://huggingface.co/docs/transformers/model_doc/bart\n",
    "\n",
    "### Результат:\n",
    "\n",
    "- код .py, .ipynb\n",
    "- выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b04db-8322-46ba-93ae-8a71dfe5cf99",
   "metadata": {},
   "source": [
    "### Загрузка библиотек и блок используемых функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1651a6dc-6325-4137-8b5b-ea08942fca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (6.30.1)\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip install sentencepiece --quiet\n",
    "!pip install protobuf --quiet\n",
    "!pip install --upgrade protobuf\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer, MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b7a450-8935-457f-b426-283b12448a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка csv полученного из парсинга json и удаленеие строк, в которых столбец text пустой\n",
    "\n",
    "def load_df(file, index=None):\n",
    "    pth1 = os.path.join('data', 'example', file)\n",
    "    pth2 = file\n",
    "\n",
    "    if os.path.exists(pth1):\n",
    "        df = pd.read_csv(pth1, na_values=np.nan)\n",
    "    elif os.path.exists(pth2):\n",
    "        df = pd.read_csv(pth2, na_values=np.nan)\n",
    "    else:\n",
    "        print('Что-то пошло не так')\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = df.dropna(subset=['text'])\n",
    "    except:\n",
    "        print('Столбец \"text\" отсутствует в датафрейме')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6bfe26c-3230-4a20-b048-e86089b39889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем сообщения на части (чанки) по заданному количеству слов\n",
    "\n",
    "def split_on_chunks(df, max_words_per_chunk=500):\n",
    "    current_words = 0\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Считаем количество слов в текущем сообщении\n",
    "        words_in_message = len(row['text'].split())\n",
    "        \n",
    "        # Если сообщение превышает лимит, разбиение на несколько частей\n",
    "        if words_in_message > max_words_per_chunk:\n",
    "            # Разбиваем длинное сообщение на несколько частей\n",
    "            words = row['text'].split()\n",
    "            while len(words) > max_words_per_chunk:\n",
    "                part = ' '.join(words[:max_words_per_chunk])\n",
    "                chunks.append([part])  # Добавляем эту часть как отдельный чанк\n",
    "                words = words[max_words_per_chunk:]\n",
    "            # Оставшуюся часть добавляем как последний чанк\n",
    "            if words:\n",
    "                chunks.append([' '.join(words)])\n",
    "        elif current_words + words_in_message > max_words_per_chunk:\n",
    "            # Если добавление сообщения превысит лимит, добавляем текущий чанк\n",
    "            chunks.append(current_chunk)\n",
    "            # Начинаем новый чанк с текущего сообщения\n",
    "            current_chunk = [row['text']]\n",
    "            current_words = words_in_message\n",
    "        else:\n",
    "            # Добавляем сообщение к текущему чанку\n",
    "            current_chunk.append(row['text'])\n",
    "            current_words += words_in_message\n",
    "    \n",
    "    # Добавляем последний чанк, если он есть\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb435439-b47c-4d22-900d-b15cbd056ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef summarize_chunk(chunk, tokenizer, model):\\n    # Объединяем сообщения части в один текст\\n    text = \\' \\'.join(chunk)\\n    \\n    # Форматируем для модели\\n    input_text = \"summarize: \" + text\\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\\n    \\n    # Генерируем резюме\\n    summary_ids = model.generate(inputs, num_beams=3, max_length=150, min_length=50, length_penalty=1.5, early_stopping=True)\\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функция суммаризации текста\n",
    "\"\"\"\n",
    "def summarize_chunk(chunk, tokenizer, model):\n",
    "    # Объединяем сообщения части в один текст\n",
    "    text = ' '.join(chunk)\n",
    "    \n",
    "    # Форматируем для модели\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Генерируем резюме\n",
    "    summary_ids = model.generate(inputs, num_beams=3, max_length=150, min_length=50, length_penalty=1.5, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fbfd1f-baf8-4209-8c48-c0dd7cc34f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(chunk, tokenizer, model):\n",
    "    # Объединяем сообщения части в один текст\n",
    "    text = ' '.join(chunk)\n",
    "    \n",
    "    # Форматируем для модели\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
    "    \n",
    "    # Генерируем резюме\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, min_length=50, length_penalty=2, early_stopping=True)\n",
    "    \n",
    "    # Декодируем результат\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summary = summary.replace(\"summarize: \", \"\") # убираем \"summarize: \" для BART\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1098eb-88b6-4198-a7c0-9e4112f0d4c9",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685fd85c-777d-4e7c-b6e0-dfede3ddff55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>chat_name</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-03T11:28:38</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user312724902</td>\n",
       "      <td>Olga Varavina</td>\n",
       "      <td>Всем большой привет! Приглашаю на свой уютный ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-03T11:52:20</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user1349934990</td>\n",
       "      <td>Илья</td>\n",
       "      <td>А у тебя когда будет свой канал про аналитику?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-03T11:52:37</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user1349934990</td>\n",
       "      <td>Илья</td>\n",
       "      <td>Будешь туда голосовухи пятиминутные постить</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-02-03T11:55:09</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user60031833</td>\n",
       "      <td>Sergey</td>\n",
       "      <td>Потому что сделаны так, будто устарели уже лет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-02-03T11:56:57</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user60031833</td>\n",
       "      <td>Sergey</td>\n",
       "      <td>Подкаст?)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date              chat_name     chat_id       sender_id  \\\n",
       "2  2025-02-03T11:28:38  💬 Data Practicum Chat  1379846874   user312724902   \n",
       "3  2025-02-03T11:52:20  💬 Data Practicum Chat  1379846874  user1349934990   \n",
       "4  2025-02-03T11:52:37  💬 Data Practicum Chat  1379846874  user1349934990   \n",
       "5  2025-02-03T11:55:09  💬 Data Practicum Chat  1379846874    user60031833   \n",
       "6  2025-02-03T11:56:57  💬 Data Practicum Chat  1379846874    user60031833   \n",
       "\n",
       "        username                                               text  \n",
       "2  Olga Varavina  Всем большой привет! Приглашаю на свой уютный ...  \n",
       "3           Илья     А у тебя когда будет свой канал про аналитику?  \n",
       "4           Илья        Будешь туда голосовухи пятиминутные постить  \n",
       "5         Sergey  Потому что сделаны так, будто устарели уже лет...  \n",
       "6         Sergey                                          Подкаст?)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_df('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205c82df-7d70-4250-bd00-83bc69a99475",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_on_chunks(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea28c34-ae38-44e3-a60a-297b6662ce29",
   "metadata": {},
   "source": [
    "### Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ec6726-dda0-46cb-9107-c69f60f091fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"google/mt5-small\", legacy=False)\n",
    "model_t5 = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0f53d9-92b2-4192-9cf1-1e14606c4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bart = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ru_RU\", tgt_lang=\"ru_RU\")\n",
    "model_bart = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f85765-a667-421f-9fcd-92886c0cfbb0",
   "metadata": {},
   "source": [
    "### Суммаризация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9860ee5-9a94-46f2-8ce2-f6debcee574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_t5 = [summarize_chunk(chunk, tokenizer_t5, model_t5) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b84dd66-8e8f-4993-b349-4b24bde3a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_bart = [summarize_chunk(chunk, tokenizer_bart, model_bart) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d97859f-8887-4007-90d6-8886e30b3fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0> спасибо))))))))))))))))))))))))))))))))))))))))))))))))))))))))))):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)):)',\n",
       " '<extra_id_0> и т.д.))))))))))))))))))))))))))))))))))))))))))))))))))) ().) )) )) )) )) )) )) )) ))) ))) )) ))) ))) ))) ))) ))))))))))))))))))))))))))))))))))))))',\n",
       " '<extra_id_0> не знаю. Интересно, а написать в сопровождение. Инсайты. Можно в любом формате. Может. можно только в любом формате. Может. можно.',\n",
       " '<extra_id_0>? Ответ очевиден. Ответ очевиден. Ответ очевиден. Ответ очевиден. Ответы не сбылись. Ответы не сбылись. Ответы не сбылись.',\n",
       " '<extra_id_0> спасибо)))))))))))))))))))))))))))))))))))))))))))))))))',\n",
       " '<extra_id_0> ссылка неактивна и все тут уже нет вопросов))))))))))))))))))))))))))))))))))))))))))))))))))))))',\n",
       " '<extra_id_0> Спасибо Ире))))))))))))))))))))))))))))))))))))))))) )))))))))',\n",
       " '<extra_id_0> спасибо) Спасибо)))))))))))))))))))))))))))))))))))))))))))))))))))))):))))))))))))))))))))))))))))))))()))))))))))))()))))))))))))',\n",
       " '<extra_id_0> спасибо))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))',\n",
       " '<extra_id_0>. Спасибо. Спасибо. Спасибо. Спасибо. Спасибо. Спасибо. Спасибо. Благодарю вас. Спасибо. Благодарил!! <extra_id_8>. <extra_id_8>. <extra_id_8>. <extra_id_52>. <extra_id_52>. <extra_id_8>.!!',\n",
       " '<extra_id_0> я так и не считаю))))))))))))))))))))))))))))))))))))))))))))))))):)):)):)):)):)) <extra_id_56>',\n",
       " '<extra_id_0> выше - стеб, кмк, есть инфа, есть инфа, есть инфа, есть инфа, есть инфа, есть инфа, есть инфа, есть инфа, есть инфа, есть инфа',\n",
       " '<extra_id_0> и т.д. и т.д.))\")\"\"\")\"\")\"))\"( )() <extra_id_10>)))))))))))))))',\n",
       " '<extra_id_0> я.. . . . . . . . . . . . . . . . . . . . . . ..',\n",
       " '<extra_id_0> и т.д.)))»)))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))',\n",
       " '<extra_id_0> яндекс практикум. Курсы яндекс практикума. Курсы яндекс практикума. Курсы яндекс практикумы. Курсы яндекс практикумы. Курсы яндекс практикума',\n",
       " '<extra_id_0> работать не будет. Вот и я.Практикум. В общем спасибо. Спасибо. Всем привет. Спасибо. :D. ) <extra_id_10>. <extra_id_51>.. <extra_id_51>.Показате.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af72e1a-e916-4912-b0b5-41e82265f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Всем большой привет! Приглашаю на свой уютный канал Диванные данные. Присаживайтесь поудобнее, обсудим вкатывание (или скатывание) в аналитику, прохождение собесов и многое другое. https://t.me/Divan_data А у тебя когда будет свой канал про аналитику? Будешь туда голосовухи пятиминутные постить Потому что сделаны так, будто устарели уже лет на цать, особенно кажется так по поводу ошибки с дизайном времен xp. Но новый логотип мне все равно нравится) Подкаст?) Не, это не так раздражает Нужны голосов',\n",
       " 'Предложение поучаствовать в бета-тестировании Продуктового аналитика и BI-аналитика *для выпускников курсов \"Аналитик данных\", \"Аналитик данных расширенный\" и \"Аналитик данных Bootcamp\" Всем привет! На связи команда курсов Аналитики данных. Мы активно работаем над курсами Продуктового аналитика и BI-аналитика, и сейчас мы в поиске бета-тестировщиков новых курсов. Для тестирования необходимы навыки в аналитике данных, именно поэтому мы пришли к вам — нашим выпускникам #бета-тест — это возможность бесплатно пройти обучение с сопровождением в Яндекс Практи',\n",
       " 'я написал в саппорт =D + Все уже обозначили, что ссылка на Nda неактивна? Щас проверю и напишу Не могу подписать NDA \"Ссылка неактивна. Запросите новую ссылку\"... Давай уже первые подкасты, а то начнутся отписки Отписки начнутся, когда пойдут мои подкасты Подписался Вообще, было бы неплохо, если все участники записывали туда какие- то кейсы рабочие. Инсайты. Можно в любом формате. Написал в сопровождение, но там по прежнему тишина:) Хочешь чтобы все годосовухи писали? Отправил заявку,',\n",
       " 'Ну наконец то! Мне кажется там на курсах теперь говорят \"Не создал канал - работать не начнешь\" Твой когда будет? Мне он не нужен, зачем Думаешь, вместо гитхаба, теперь оформляют свой канал?) А коллективный разум будет чем мотивирован, чтобы делать эту работу? (Это именно работа, причем трудная) Думаю да, ведь бесполезней гитхаба совета сложно придумать Чтобы было проще подготовиться к собеседованию/тестовому, зная, что тебя может ждать @umagalova у тебя уже 12 подписчиков, когда контент? Чтобы подготовиться, нужно чтобы что-то было там, откуда оно будет появляться?',\n",
       " 'может быть, эмоциональные качели и правда есть в этом состоянии, но если брать этот принцип как единственно верный и опираться только на него все же течет, все меняется а у тебя в жизни был опыт курсов по психологии или я какой-то хрестоматийный пример выдал во фразе не, скорее из личного опыта терапии личной и групповой) мы с моим психологом пока пришли к тому, что у меня есть признаки нарцисса, возможно, что дойдем и до биполярки а ты с каким запросом приходила а нарциссизм есть у всех) напиши мне в личку) ну да, но он',\n",
       " 'да это не Яндекс и не Бигтек :) Ритейл можно сказать и не Яндекс, а Бигтек :) У меня с одной стороны маркетологи, а с другой я делаю и учусь делать прогнозы как фин аналитик в экселе (в гробу это видал, но увы) каким мылом пользуешься? яндекс-почта, гмейл а ты хорош за эти месяцы познал кубы, репортинг сервис, мс студио, питон-пандас тоже использую, даже z-score пригодился (ну и еще внутренне, там оракл bi, но я там особо не хожу) а',\n",
       " 'Если ты уже прошёл учебник (сдал резюме и сопроводительное письмо) — в Акселерацию тоже можно заходить в течение 6 месяцев после окончания основного курса (ДА+) Вся информация о переходе в Акселерацию лежит в уроке «К поиску работы» Понял спасибо! /toprep@yndxcbot спасибо Спасибо) Оживили чат) и тебе спасибо) /toprep@yndxcbot /toprep@yndxcbot Чот слишком высокую планку установили, ящитаю Спасибо @super_ira за высокую планку грешу этим, да Спасибо тебе за это Спасибо Ире) https://t.me/',\n",
       " 'Спасибо) А если запустить предсказ бота Предсказание для @george5555: Ожидай приятных новостей, но не слишком сильно. после тебя я не в Вьетнаме) Я тоже Зря Ребята, всем привет! Кто-нибудь пробовал использовать модель T-банка для рекомендаций? Я отправил в неё свои данные и обучил - результат нормальный. Теперь как-то надо сохранить эту модель. Не могу разобраться в коде. Помогите, плиз)) Ссылка на скрипт обучения https://github.com/Nemexur/revisit-bpr/blob/main/experiments',\n",
       " 'всех с праздником, зайки! https://ryba.team/courses/clean-butt И как мы раньше жили без этого курса теперь жопа точно будет чистой спасибо! мусульманам с рождения дали доступ на этот курс кстати /toprep@yndxcbot /toprep@yndxcbot /toprep@yndxcbot /toprep@yndxcbot /toprep@yndxcbot /toprep@yndxcbot /toprep@yndxcbot Спасибо) Пошёл седьмой год Практикума Ура, мы это сделали — выпускники и амбассадоры разных курсов',\n",
       " 'Никакой. И еще, не понятен смысл кнопок, если карточка открывается только при наведении на название. Это ведь кнопки должны быть? Когда я вожу курсором по квадратикам, они подсвечиваются Может не прогружается? У меня все открывается И там нет кнопок, это ведь собрано в виде борды с карточками Проверил снова, заработало. а клоуна за что Блин, вообще не планировался такая реакция похоже случайно зажала Оставь Как-будто не за что Хотел протестировать ГроуФуд, раз уж скидка есть, но это оказалось полной шляпой. Делюсь опыт',\n",
       " 'Нет. Там техническое интервью и интервью с командой или руководством Ну 2 года назад так было Звучит как анализы сдать Щас все HR расстроились, что их за людей не считают. И беседа с ними - не собеседование Не везде. Hr собес я не считаю. Это разговор по телефону длиною в 5 минут) или в тг спросит че как готова?) Не забыв приложить ссыль на вакансию У меня с hR полноценно по 30-45 минут встречи идут. Я вопросы задаю не меньше Hr Вообщем, последний раз когда была в поисках, у меня только один собес состоял из двух этапов Так они',\n",
       " 'Пет-проекты для аналитиков: как наполнить портфолио рабочими кейсами Каким пет-проектом заняться аналитику данных зимой? Вариантов много. Например, сделать инфографику самых высоких городских ёлок или разработать бота, который посчитает, какого объёма нужен таз под оливье. А вот зачем и как правильно создавать пет-проекты — читайте в статье по ссылке. Можно узнать, есть какие-то новости по этому проекту? Вроде бы, собирались стартовать в середине февраля предполагаю, что уже идет.... Должны были добавить в чат и расшарить доступ в личном',\n",
       " 'Сильная связь с матерью, в отличии от @super_ira, у которой большая связь с отцом, но он рано ушёл из её жизни.... Сериал продолжается... Кстати, Ксения намного лучше развита духовно... Сегодня родительская суббота, позвоните родным спасибо) Спасибо) Доброе утро, все сообщения выше были удалены, хочу напомнить о правилах чата! Мы не обсуждаем политику, религию, не призываем к насилию и не распространяем информацию, которая может оскорбить участников чата. Прошу соблюдать правила, а не сеять негатив и злобу в чате Кто-то был на синей',\n",
       " 'Самое смешное что у меня так и получилось Понимаю тебя Случайно откликнулся на вакансию программиста и меня взяли) В первом пункте « у нас нет правил» Общечеловеческие правила в сообществе никто не отменял. На этом тему можно закрыть. Тяжко с душными аналитиками?) Предлагаю отгадать ребус А=я Пишите ответы реплаем к сообщению Сяп Но Иру не догонишь Когда знала в чем подкол, но все равно идёшь на поводу... Зачем ты поддаешься?) Я в итоге разработкой занимаюсь)) аналитики минимум Ну человек хотел услышать',\n",
       " 'А я в мск вернулась Ну тем более Ну скидывай реквизиты куда кидать Можешь в тонкоинках на телегу кинуть, я нежадный могу в трамп коинах кинуть Не, это для наивных Ты ему тоже должен ни ща что? Просто так? Не пользуюсь я ему плачу чтобы он дружил со мной Кайф Заодно научишься База А я умею Тогда жду перевода А ты мне что? Почет и уважение Не интересует Ну «берешь» громко сказано пока никто не отдал Не было такого вопроса) 3 марта стартует новый поток Карьерного трека. Привет, на связи команда труд',\n",
       " 'Я писала об этом куратору в качестве обратной связи. Коротко напишу здесь. Когда я выбирала курс, расчитывала на одно: узнавала про программу карьерного трека и как там всё устроено. Потом оказалось, что всё не так, как писали, и мне тот карьерный трек, про который рассказывают \"не положен\". В итоге после обучения на курсе я осталась с обманутыми ожиданиями. И фразы типа \" ВСЕМ выпускникам предоставляется доступ к Карьерному треку\" с конкретным описанием того, что туда входит могут привести к таким же обманутым ожиданиям у других выпускников Пра',\n",
       " 'Так же ди обучался. Три дня на коннекторы s3 положил, оч вкутно (нет) У вк Клауд сегодня вебинар по с3, кст С коннекторами, вроде, у меня лично проблем не было. Но у меня было куча проблем с тем, что из-за непонятных заданий у меня расходились данные с тем, что ожидал тренажёр, переставали проходить проверки. При этом тебе изначально предоставляют довольно большую свободу в выборе подходов. А через несколько разделов оказывается, что надо было сделать единственно верным способом, и продолжить из-за этого ты не можешь. Возвращаешься назад']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_bart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915d25d-f414-47d1-9bc3-20ea73a90c4b",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "\n",
    " - Попробовал использовать для сумаризации:\n",
    "     - mT5 - многоязыковую версию T5, предварительно обученную на многоязыковом корпусе Common Crawl (mC4), охватывающем 101 язык;\n",
    "     - mBART-50\tмногоязыковую версию BART, предварительно обученную на 50 языках.\n",
    " - mT5 на выхожде не дает осмысленного текста и добавляет специальные токены, такие как, например, 'extra_id_0', не смотря на то, что установил параметр, который должен их убирать skip_special_tokens=True.\n",
    " - mBART-50 дала хорошший результат.\n",
    " - Использовались небольшие модели mt5-small и mbart-large-50, но и на них суммаризация на офисном ноутбуке проиходит очень медленно.\n",
    " - Возможно, следует использовать для задач суммаризации нейронки через API, если нет ресурсов для локального использования более мощных моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1ec93-a3e4-4d2b-b228-77f9d9f3a4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
