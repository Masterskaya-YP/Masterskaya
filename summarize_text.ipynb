{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f8d361-1615-4da0-a6d6-59848e338ba9",
   "metadata": {},
   "source": [
    "# Абстрактивная суммаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd3608-5e6c-42f7-bada-463d68d7b1de",
   "metadata": {},
   "source": [
    "### Контекст\n",
    "\n",
    "Необходимо провести суммаризацию полученной информации методами абстрактивной сумммаризации.\n",
    "\n",
    "Абстрактивная суммаризация – это метод автоматического реферирования текста, при котором создается новое краткое изложение с использованием перефразирования и генерации новых предложений, а не просто извлечения фрагментов из исходного текста.\n",
    "\n",
    "### Что нужно сделать?\n",
    "\n",
    "Попробовать разыне методы абстрактивной суммаризации:\n",
    "\n",
    "- T5 (Text-to-Text Transfer Transformer) - https://github.com/google-research/text-to-text-transfer-transformer\n",
    "- BART https://huggingface.co/docs/transformers/model_doc/bart\n",
    "\n",
    "### Результат:\n",
    "\n",
    "- код .py, .ipynb\n",
    "- выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b04db-8322-46ba-93ae-8a71dfe5cf99",
   "metadata": {},
   "source": [
    "### Загрузка библиотек и блок используемых функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1651a6dc-6325-4137-8b5b-ea08942fca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (6.30.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (4.50.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agsav\\anaconda3\\envs\\practicum\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip install sentencepiece --quiet\n",
    "!pip install protobuf --quiet\n",
    "!pip install --upgrade protobuf\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration, \n",
    "    T5Tokenizer, \n",
    "    MBartForConditionalGeneration, \n",
    "    MBart50TokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoModelForCausalLM)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Для метрик\n",
    "!pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b7a450-8935-457f-b426-283b12448a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка csv полученного из парсинга json и удаленеие строк, в которых столбец text пустой\n",
    "\n",
    "def load_df(file, index=None):\n",
    "    pth1 = os.path.join('data', 'example', file)\n",
    "    pth2 = file\n",
    "\n",
    "    if os.path.exists(pth1):\n",
    "        df = pd.read_csv(pth1, na_values=np.nan)\n",
    "    elif os.path.exists(pth2):\n",
    "        df = pd.read_csv(pth2, na_values=np.nan)\n",
    "    else:\n",
    "        print('Что-то пошло не так')\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = df.dropna(subset=['text'])\n",
    "    except:\n",
    "        print('Столбец \"text\" отсутствует в датафрейме')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6bfe26c-3230-4a20-b048-e86089b39889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем сообщения на части (чанки) по заданному количеству слов\n",
    "\n",
    "def split_on_chunks(df, max_words_per_chunk=500):\n",
    "    current_words = 0\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Считаем количество слов в текущем сообщении\n",
    "        words_in_message = len(row['text'].split())\n",
    "        \n",
    "        # Если сообщение превышает лимит, разбиение на несколько частей\n",
    "        if words_in_message > max_words_per_chunk:\n",
    "            # Разбиваем длинное сообщение на несколько частей\n",
    "            words = row['text'].split()\n",
    "            while len(words) > max_words_per_chunk:\n",
    "                part = ' '.join(words[:max_words_per_chunk])\n",
    "                chunks.append([part])  # Добавляем эту часть как отдельный чанк\n",
    "                words = words[max_words_per_chunk:]\n",
    "            # Оставшуюся часть добавляем как последний чанк\n",
    "            if words:\n",
    "                chunks.append([' '.join(words)])\n",
    "        elif current_words + words_in_message > max_words_per_chunk:\n",
    "            # Если добавление сообщения превысит лимит, добавляем текущий чанк\n",
    "            chunks.append(current_chunk)\n",
    "            # Начинаем новый чанк с текущего сообщения\n",
    "            current_chunk = [row['text']]\n",
    "            current_words = words_in_message\n",
    "        else:\n",
    "            # Добавляем сообщение к текущему чанку\n",
    "            current_chunk.append(row['text'])\n",
    "            current_words += words_in_message\n",
    "    \n",
    "    # Добавляем последний чанк, если он есть\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61fbfd1f-baf8-4209-8c48-c0dd7cc34f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общая функция суммаризации текста\n",
    "def summarize_chunk(chunk, tokenizer, model, prefix=''):\n",
    "    # Объединяем сообщения части в один текст\n",
    "    text = ' '.join(chunk)\n",
    "    \n",
    "    # Форматируем для модели\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"longest\")\n",
    "    \n",
    "    # Генерируем резюме\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, min_length=50, length_penalty=2, early_stopping=True)\n",
    "    \n",
    "    # Декодируем результат\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summary = summary.replace(\"summarize: \", \"\") # убираем \"summarize: \" для BART\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449c7bdc-d8bc-4109-87fa-aaf74eaa64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отдельная функция для ruGPT3\n",
    "def summarize_with_rugpt3(text, tokenizer, model, max_length=150):\n",
    "    prompt = f\"Текст: {text.strip()}\\nКраткое содержание:\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        num_beams=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    summary_start = generated.find(\"Краткое содержание:\")\n",
    "    return generated[summary_start + len(\"Краткое содержание:\"):].strip() if summary_start != -1 else generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262a58d7-4982-4d76-9af6-86f064c99810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для расчета сходства\n",
    "def calculate_similarity(original_text, summary_text):\n",
    "    # модель SentenceTransformer, и метод encode преобразует текст в числовое представление\n",
    "    model_st = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    emb_src = model_st.encode(original_text, convert_to_tensor=True) \n",
    "    emb_sum = model_st.encode(summary_text, convert_to_tensor=True)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    cos_sim = util.pytorch_cos_sim(emb_src, emb_sum)\n",
    "    cos_sim_score = cos_sim.item()\n",
    "    \n",
    "    return cos_sim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1098eb-88b6-4198-a7c0-9e4112f0d4c9",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685fd85c-777d-4e7c-b6e0-dfede3ddff55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>chat_name</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-03T11:28:38</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user312724902</td>\n",
       "      <td>Olga Varavina</td>\n",
       "      <td>Всем большой привет! Приглашаю на свой уютный ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-03T11:52:20</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user1349934990</td>\n",
       "      <td>Илья</td>\n",
       "      <td>А у тебя когда будет свой канал про аналитику?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-03T11:52:37</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user1349934990</td>\n",
       "      <td>Илья</td>\n",
       "      <td>Будешь туда голосовухи пятиминутные постить</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-02-03T11:55:09</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user60031833</td>\n",
       "      <td>Sergey</td>\n",
       "      <td>Потому что сделаны так, будто устарели уже лет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-02-03T11:56:57</td>\n",
       "      <td>💬 Data Practicum Chat</td>\n",
       "      <td>1379846874</td>\n",
       "      <td>user60031833</td>\n",
       "      <td>Sergey</td>\n",
       "      <td>Подкаст?)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date              chat_name     chat_id       sender_id  \\\n",
       "2  2025-02-03T11:28:38  💬 Data Practicum Chat  1379846874   user312724902   \n",
       "3  2025-02-03T11:52:20  💬 Data Practicum Chat  1379846874  user1349934990   \n",
       "4  2025-02-03T11:52:37  💬 Data Practicum Chat  1379846874  user1349934990   \n",
       "5  2025-02-03T11:55:09  💬 Data Practicum Chat  1379846874    user60031833   \n",
       "6  2025-02-03T11:56:57  💬 Data Practicum Chat  1379846874    user60031833   \n",
       "\n",
       "        username                                               text  \n",
       "2  Olga Varavina  Всем большой привет! Приглашаю на свой уютный ...  \n",
       "3           Илья     А у тебя когда будет свой канал про аналитику?  \n",
       "4           Илья        Будешь туда голосовухи пятиминутные постить  \n",
       "5         Sergey  Потому что сделаны так, будто устарели уже лет...  \n",
       "6         Sergey                                          Подкаст?)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_df('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205c82df-7d70-4250-bd00-83bc69a99475",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_on_chunks(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea28c34-ae38-44e3-a60a-297b6662ce29",
   "metadata": {},
   "source": [
    "### Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ec6726-dda0-46cb-9107-c69f60f091fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"google/mt5-small\", legacy=False)\n",
    "model_t5 = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0f53d9-92b2-4192-9cf1-1e14606c4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bart = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ru_RU\", tgt_lang=\"ru_RU\")\n",
    "model_bart = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6009bd4b-6547-4a5e-8478-269d8084c9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer_rut5 = AutoTokenizer.from_pretrained(\"cointegrated/rut5-small\")\n",
    "model_rut5 = AutoModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01179833-9bed-4f47-90b6-d25c1d05d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt3 = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3large_based_on_gpt2\")\n",
    "model_gpt3 = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3large_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b95a94-63b2-4802-a831-44169ec8edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_rut5_bsg = AutoTokenizer.from_pretrained(\"IlyaGusev/rut5_base_sum_gazeta\")\n",
    "model_rut5_bsg = AutoModelForSeq2SeqLM.from_pretrained(\"IlyaGusev/rut5_base_sum_gazeta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f85765-a667-421f-9fcd-92886c0cfbb0",
   "metadata": {},
   "source": [
    "### Суммаризация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a6944d-ab8c-42bc-ab95-d9660f7d6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для ускорения вывода и проверки использую срез чанков chunks[1:3], если нужно по всем, убери [1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404d3fef-c0a8-4099-a014-edd032063021",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_bart = [summarize_chunk(chunk, tokenizer_bart, model_bart) for chunk in chunks[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0026a1-1d8d-472f-9bce-b5f420c7e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_t5 = [summarize_chunk(chunk, tokenizer_t5, model_t5, prefix=\"summarize: \") for chunk in chunks[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0c6f17-8003-4b3a-9cb4-0e3fa7e163ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_rut5 = [summarize_chunk(chunk, tokenizer_rut5, model_rut5, prefix=\"summarize: \") for chunk in chunks[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40015de-9d8b-4672-93a5-23ac806c24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_rut5_bsg = [summarize_chunk(chunk, tokenizer_rut5_bsg, model_rut5_bsg, prefix=\"summarize: \") for chunk in chunks[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190e859a-e136-4576-be97-966fd75083dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_summaries_gpt3 = [summarize_with_rugpt3(' '.join(chunk), tokenizer_gpt3, model_gpt3) for chunk in chunks[1:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc8972e1-7a92-4b42-ac08-b6191a5125f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0> и т.д.))))))))))))))))))))))))))))))))))))))))))))))))))) ().) )) )) )) )) )) )) )) ))) ))) )) ))) ))) ))) ))) ))))))))))))))))))))))))))))))))))))))',\n",
       " '<extra_id_0> не знаю. Интересно, а написать в сопровождение. Инсайты. Можно в любом формате. Может. можно только в любом формате. Может. можно.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e50be3c-a7f0-4a17-8ec6-05fc6ac94ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Предложение поучаствовать в бета-тестировании Продуктового аналитика и BI-аналитика *для выпускников курсов \"Аналитик данных\", \"Аналитик данных расширенный\" и \"Аналитик данных Bootcamp\" Всем привет! На связи команда курсов Аналитики данных. Мы активно работаем над курсами Продуктового аналитика и BI-аналитика, и сейчас мы в поиске бета-тестировщиков новых курсов. Для тестирования необходимы навыки в аналитике данных, именно поэтому мы пришли к вам — нашим выпускникам #бета-тест — это возможность бесплатно пройти обучение с сопровождением в Яндекс Практикуме. Главная',\n",
       " 'я написал в саппорт =D + Все уже обозначили, что ссылка на Nda неактивна? Щас проверю и напишу Не могу подписать NDA \"Ссылка неактивна. Запросите новую ссылку\"... Давай уже первые подкасты, а то начнутся отписки Отписки начнутся, когда пойдут мои подкасты Подписался Вообще, было бы неплохо, если все участники записывали туда какие- то кейсы рабочие. Инсайты. Можно в любом формате. Написал в сопровождение, но там по прежнему тишина:) Хочешь чтобы все годосовухи писали? Отправил заявку, но без NDA']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f480b39-0d40-4ed6-9bfe-05e7e77b1af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Что нужно делать тестировщику, которая может улучшить процесс обучения, именно поэтому мы пришли к тебе в бета-тестировании по материалам, которая может улучшить процесс учёбы, именно поэтому мы в поиске бета-тестировщиков, именно поэтому мы в поиске бета-тестировщиков, именно поэтому мы в поиске бета-тестировщиков, именно поэтому мы в поиске бета-тестировщиков, именно поэтому мы сможем выдать дипломы о прохождении курса, которая может улучшить процесс учёбы',\n",
       " 'Что может быть неплохо, если все участники записывали туда какие-то кейсы рабочие, Инсайты, а тоже отправила заявку без NDA, как только оставила заявку без NDA, как только оставила заявку без NDA, как только оставила заявку без NDA, как только оставила заявку без ссылки, но не могу подписать отправил заявку, но не могу подписать, как только можно подписать, как только можно подписать, как только можно подписать, как только можно подписать, как только можно подписать,']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_rut5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a52f0be6-4ef4-4424-893f-0fd3a5794df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Выпускники курсов Продуктового аналитика и BI-аналитика получат возможность бесплатно пройти бета-тестирование Продуктового аналитика и BI-аналитика с сопровождением в Яндекс Практикуме. Тестирование будет проходить с середины февраля 2025 по начало июня 2025.',\n",
       " 'Если все участники курса LLM будут записывать свои годосовухи, то начнутся отписки, когда пойдут мои подкасты, а то начнутся отписки, когда пойдут мои подкасты.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_rut5_bsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07d22d8b-f94c-457d-b4b8-d85dc1e9bbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Исходный код продукта\" - это набор инструкций, кодов и инструкторов, который помогает оптимизировать бизнес и производительность бизнес процессов. При разработке продуктов, разработчики обычно используют набор алгоритмов. Каждый алгоритм в свою очередь, является результатом обработки информации от различных источников, таким образом, все эти алгоритмы и инструкции должны быть связаны между собой.\\nДанный код описывает последовательность шагов, необходимых для выполнения алгоритма. Код - часть алгоритмической структуры, поэтому он должен быть реализован таким, каким он описан. Алгоритм - набор данных - инструктирующих, управляющих и управляющих, предназначенных для определения последовательности выполнения. В большинстве случаев, алгоритм может быть представлен в двух видах: алгоритм и инструкция. Каждое из этих понятий описывает способ',\n",
       " 'Мы все учились, мы все работаем.\\nВсе ли знают, чего они хотят?\\nПсихологический портрет на основании результатов исследования. Выявление сильных и слабых сторон. Как с помощью статистики понять свою задачу на рынке труда? Как повысить свою продуктивность? Какой из навыков является самым эффективным и выгодным для работодателя? Какие навыки могут помешать найти хорошую работу? \\n\\n1) Как определить свои сильные стороны? \\n2) Какие ошибки совершают при подборе кандидатов? Что делает человек, который не знает своих сильных сторон? Где он их ищет? Кто может их раскрыть? И что делать,если он видит их в другом человеке? (об этом я писал в статье \"Почему я не люблю работать\" httpss://vk.com/w']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_summaries_gpt3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a6e4e-8807-457a-85cf-9ee29b65dcc4",
   "metadata": {},
   "source": [
    "### Метрики для суммаризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edab95-88d9-4762-9b19-a82d8d6c8ff0",
   "metadata": {},
   "source": [
    "Cosine Similarity (косинусная мера схожести)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9d9ad36-19bf-491f-bd45-44d952895f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Chunk\": [],\n",
    "    \"BART\": [],\n",
    "    \"T5\": [],\n",
    "    \"rut5\": [],\n",
    "    \"rut5_bsg\": [], \n",
    "    \"rugpt3\": []\n",
    "}\n",
    "\n",
    "for i in range(1, 3):\n",
    "    original_text = ' '.join(chunks[i-1])\n",
    "    \n",
    "    summary_bart = chunk_summaries_bart[i-1]\n",
    "    cos_sim_bart = calculate_similarity(original_text, summary_bart)\n",
    "    \n",
    "    summary_t5 = chunk_summaries_t5[i-1]\n",
    "    cos_sim_t5 = calculate_similarity(original_text, summary_t5)\n",
    "    \n",
    "    summary_rut5 = chunk_summaries_rut5[i-1]\n",
    "    cos_sim_rut5 = calculate_similarity(original_text, summary_rut5)\n",
    "    \n",
    "    summary_rut5_bsg = chunk_summaries_rut5_bsg[i-1]\n",
    "    cos_sim_rut5_bsg = calculate_similarity(original_text, summary_rut5_bsg)\n",
    "    \n",
    "    summary_gpt3 = chunk_summaries_gpt3[i-1]\n",
    "    cos_sim_gpt3 = calculate_similarity(original_text, summary_gpt3)\n",
    "    \n",
    "    results[\"Chunk\"].append(f\"Chunk {i}\")\n",
    "    results[\"BART\"].append(cos_sim_bart)\n",
    "    results[\"T5\"].append(cos_sim_t5)\n",
    "    results[\"rut5\"].append(cos_sim_rut5)\n",
    "    results[\"rut5_bsg\"].append(cos_sim_rut5_bsg)\n",
    "    results[\"rugpt3\"].append(cos_sim_gpt3)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25bb9350-fb65-42e2-a2cb-b829258ee3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>BART</th>\n",
       "      <th>T5</th>\n",
       "      <th>rut5</th>\n",
       "      <th>rut5_bsg</th>\n",
       "      <th>rugpt3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 1</td>\n",
       "      <td>0.436778</td>\n",
       "      <td>0.225437</td>\n",
       "      <td>0.204232</td>\n",
       "      <td>0.279810</td>\n",
       "      <td>0.266572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chunk 2</td>\n",
       "      <td>0.251637</td>\n",
       "      <td>0.157351</td>\n",
       "      <td>0.178092</td>\n",
       "      <td>0.385551</td>\n",
       "      <td>0.498198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Chunk      BART        T5      rut5  rut5_bsg    rugpt3\n",
       "0  Chunk 1  0.436778  0.225437  0.204232  0.279810  0.266572\n",
       "1  Chunk 2  0.251637  0.157351  0.178092  0.385551  0.498198"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915d25d-f414-47d1-9bc3-20ea73a90c4b",
   "metadata": {},
   "source": [
    "### Вывод\n",
    " - Для ускорения вывода и проверки использовал срез из двух кусков(chunks).\n",
    " - Попробовал использовать для сумаризации:\n",
    "     - mT5 - многоязыковую версию T5, предварительно обученную на многоязыковом корпусе Common Crawl (mC4), охватывающем 101 язык;\n",
    "     - mBART-50\tмногоязыковую версию BART, предварительно обученную на 50 языках;\n",
    "     - rut5 - русскоязычную модель на основе модели google/mt5-small;\n",
    "     - rugpt3large_based_on_gpt2 - архитектура модели основана на GPT-2, но обучение фокусировалось на данных русского языка.\n",
    " - mT5 на выхожде не дает осмысленного текста и добавляет специальные токены, такие как, например, 'extra_id_0', не смотря на то, что установил параметр, который должен их убирать skip_special_tokens=True.\n",
    " - mBART-50 дала хорошший результат.\n",
    " - rut5 дала суммаризацию лучше чем mT5. Но есть многочисленные повторы слов в суммаризованной информации.\n",
    " - rugpt3large_based_on_gpt2 - не смотря на мои ожидания, недостаточно хорошо суммаризировала текст и добавляла служебные символы, например, \"\\n\".\n",
    " - Cosine Similarity (косинусная мера схожести) показала низкие значения, а чем ниже метрика тем тексты менее схожи. Я получил максимальную оценку на rugpt3 ~0.45 и mBART-50 ~0.44. Остальные значения меньше 0.4 и говорят о том, что тексты могут быть совершенно разными.\n",
    " - Использовались небольшие модели, но и на них суммаризация на офисном ноутбуке проиходит очень медленно.\n",
    " - Возможно, следует использовать для задач суммаризации нейронки через API, если нет ресурсов для локального использования более мощных моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595376e2-dfe4-452e-8796-4f8d5dbea7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
