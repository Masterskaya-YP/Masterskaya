### Вывод
 - Для ускорения вывода и проверки использовал срез из двух кусков(chunks).
 - Попробовал использовать для сумаризации:
     - mT5 - многоязыковую версию T5, предварительно обученную на многоязыковом корпусе Common Crawl (mC4), охватывающем 101 язык;
     - mBART-50	многоязыковую версию BART, предварительно обученную на 50 языках;
     - rut5 - русскоязычную модель на основе модели google/mt5-small;
     - rugpt3large_based_on_gpt2 - архитектура модели основана на GPT-2, но обучение фокусировалось на данных русского языка.
 - mT5 на выхожде не дает осмысленного текста и добавляет специальные токены, такие как, например, 'extra_id_0', не смотря на то, что установил параметр, который должен их убирать skip_special_tokens=True.
 - mBART-50 дала хорошший результат.
 - rut5 дала суммаризацию лучше чем mT5. Но есть многочисленные повторы слов в суммаризованной информации.
 - rugpt3large_based_on_gpt2 - не смотря на мои ожидания, недостаточно хорошо суммаризировала текст и добавляла служебные символы, например, "\n".
 - Cosine Similarity (косинусная мера схожести) показала низкие значения, а чем ниже метрика тем тексты менее схожи. Я получил максимальную оценку на rugpt3 ~0.45 и mBART-50 ~0.44. Остальные значения меньше 0.4 и говорят о том, что тексты могут быть совершенно разными.
 - Использовались небольшие модели, но и на них суммаризация на офисном ноутбуке проиходит очень медленно.
 - Возможно, следует использовать для задач суммаризации нейронки через API, если нет ресурсов для локального использования более мощных моделей.
